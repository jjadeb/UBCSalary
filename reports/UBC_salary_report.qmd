---
project:
  type: website
  output-dir: docs
title: "UBC Salaries: Exploratory Analysis of Gender"
author: "Jade Bouchard"
format: 
    html: 
        toc: true
        toc-depth: 2
    pdf: 
        toc: true
        toc-depth: 2
bibliography: references.bib
execute: 
    echo: false
    warning: false
editor: source
---

```{python}
import pandas as pd
import pickle
from IPython.display import Markdown, display
from tabulate import tabulate
```


## Aim

This document explores The University of British Columbia (UBC) faculty salaries based on guessed gender.

## Data

Salary data was sourced from @salary_data. To access individual financial reports, click on the yearly links under the header `Statement of Financial Information (SOFI)`.

Gender data was inferred using first names of staff members. In order to guess gender, I used baby name datasets [@canadian_babyname; @american_babyname; @indian_babyname].

## Ethics

In this project first names are used to guess whether someone is "male" or "female". Many people do not fit into these categories. Misgendering, or incorrectly assigning gender to individuals, can have harmful effects. In addition, while first names can sometimes be an indication of someones gender, first names are not inherintly gendered. 

I encourage anyone who notices a misgendering within this project to raise an issue in the issues tab, and it will be corrected. In addition, I encourage respectful and inclusive language in all discussions related to gender. 

## Methods

The Python programming language [@Python] was used to perform this analysis.

### Data collection

As mentioned earlier, for UBC salary data, I used the salary PDFs that UBC releases every year [@salary_data]. The following steps were taken to collect the data.

- Use the `requests` package to access the UBC Financial Reports [webpage](https://finance.ubc.ca/reporting-planning-analysis/reports-and-disclosures).
- Extract all links and filter for the Statement of Financial Information (SOFI) PDF links which contain salary information.
- If there are any PDFs from which I have not already collected salary data, extract the text from the PDF using the package `pypdf`.
- Store the new salary data

An exerpt of the raw salary data is below.

```{python}
  with open("../data/salary_data/raw_salary_data.pickle", "rb") as raw_salary_dict:
        raw_salary_text_data = pickle.load(raw_salary_dict)

print(raw_salary_text_data["2020"][200210:200300])
```


### Data cleaning

In this section, the following steps are taken to clean the salary data:

For each year of salary data,

- Remove special charachters from text. For example, ÅŸ.
- Remove unnecessary text content. For example, the "[Auditor's] Qualified Opinion".
- Uses regex to process the raw text data into a structured DataFrame with columns: `Name`, `Remuneration`, `Expenses`.
- Split the `Name` column into first and last names. 
- Convert salary (remuneration) and expenses columns to a numeric data type.
- Shorten first and last names to allow for easier name matching between years. For example, someone's name in 2020 could be "Bob M Sherbert" and in 2021 their name could be "Bob-M Sherbert". This name would be shortened to Bob Sherbert to avoid mismatching.

Then, I concatenate dataframes from all years together.

@tbl-cleandata shows an expert of the cleaned salary data.
```{python}
#| label: tbl-cleandata
#| tbl-cap: Clean UBC Salary Data
clean_salary_data = pd.read_csv("../data/salary_data/clean_salary_data/all_clean_salary_data.csv")
clean_salary_data.head()
```

## Gender Prediction 

### Babyname Corpus
In order to predict gender, I used datasets with babynames and assigned genders. In order to have a somewhat diverse set of baby names, I used babynames from Canadian, American, and Indian sources [@canadian_babyname, @american_babyname, @indian_babyname].

For each UBC staff name, I found whether that name was more common among girls or boys in the babyname corpus. Then, I guessed the gender that was most common.

In @tbl-babynames, the `Estimated_Accuracy` column shows the percentage of gender majority from the babyname corpus. For example, if 95% of babys named George in the dataset were male, the `Estimated_Accuracy` column value would be 0.95. Any staff names that had less than an 80% gender majority were given a null gender.

```{python}
#| label: tbl-babynames
#| tbl-cap: Babyname Data
clean_babynames = pd.read_csv("../data/gender_corpus/clean_name_corpus.csv")
low_accuracy_df = clean_babynames.sort_values(by="Estimated_Accuracy").head(1)
low_accuracy_name = low_accuracy_df.iloc[0]["First_Name"]
low_accuracy_value = low_accuracy_df.iloc[0]["Estimated_Accuracy"]
low_accuracy_df
```

@tbl-babynames shows the name `{python} low_accuracy_name` has about the same number of boy and girl names. This name has an accuracy of `{python} low_accuracy_value`. So, since its not above the 80% threshold, anyone with that name would have a gender value of `None` assigned.

```{python}
corpus_predictions = pd.read_csv("../data/gender_predictions/corpus_gender_predictions.csv")
non_corpus_data = pd.read_csv("../data/gender_predictions/needs_gender_predictions.csv")
matched_percentage = round(100*corpus_predictions.shape[0]/(corpus_predictions.shape[0] + non_corpus_data.shape[0]),2)
```

@tbl-corpuspreds shows some of the predictions made on UBC staff members using the babyname corpus.
```{python}
#| label: tbl-corpuspreds
#| tbl-cap: Babyname Data
corpus_predictions = pd.read_csv("../data/gender_predictions/corpus_gender_predictions.csv")
corpus_predictions_accurate = corpus_predictions[corpus_predictions["Estimated_Accuracy"] >= 0.8]
corpus_predictions_accurate[["First_Name","Sex_at_birth","Estimated_Accuracy"]].head(5)
```

```{python}
corpus_predictions = pd.read_csv("../data/gender_predictions/corpus_gender_predictions.csv")
corpus_predictions_accurate = corpus_predictions[corpus_predictions["Estimated_Accuracy"] >= 0.8]
corpus_accurate_value = round(corpus_predictions_accurate.shape[0]*100/corpus_predictions.shape[0],1)
```

Around `{python} matched_percentage`% of UBC staff names were matched with names in the babyname corpus. `{python} corpus_accurate_value`% of the corpus matches had over 0.8 accuracy and were kept, the rest were given a prediction of `None`.

### NLTK

For UBC staff names that were not found in the babyname corpus, I used a natural language processing model to predict genders [@NLTK]. 

@tbl-needspreds shows some examples of names not found in the babyname corpus.

```{python}
#| label: tbl-needspreds
#| tbl-cap: Example Staff Names Not Found in Babyname Datasets
nltk_predictions_needed = pd.read_csv("../data/gender_predictions/needs_gender_predictions.csv")
pd.DataFrame(nltk_predictions_needed.head(5)["First_Name"])
```


In order to train and evaluate the model, the babyname corpus was split into a trianing and test set.

Features used to train the Naive Bayes model were the last 2, 3, and 4 letters of the staff member's first name. 

```{python}
 with open("../data/gender_predictions/accuracy.txt", "r") as file:
    accuracy = float(file.read())
```

 The accuracy on the test set was `{python} accuracy`. However, the accuracy on the UBC staff names that were missing from the babyname corpus is very likely lower than `{python} accuracy`. This is due to the fact that the data we are making predictions on is quite different from our training data.

 Below are the top three features the classifier found most useful for making correct predictions.

```{python}
 with open("../models/gender_classifier.pickle", "rb") as model_file:
    classifier = pickle.load(model_file)
    classifier.show_most_informative_features(n=3)
```

We can see there are patterns in first names that could be helpful for predicting gender. However, these patterns may not show up often in the unique UBC staff that were not in the babyname datasets.

Finally, after making predictions on the UBC staff data, we can see in @tbl-worstnltkpreds the predictions our classifier was least confident about, and in @tbl-bestnltkpreds and the predictions it was most confident about. 

```{python}
#| label: tbl-worstnltkpreds
#| tbl-cap: Least Confident NLTK predictions
nltk_predictions = pd.read_csv("../data/gender_predictions/nltk_gender_predictions.csv")
nltk_predictions.sort_values(by="Estimated_Accuracy", ascending=True)[["First_Name", "Sex_at_birth","Estimated_Accuracy"]].drop_duplicates().head(5)
```

```{python}
#| label: tbl-bestnltkpreds
#| tbl-cap: Most Confident NLTK predictions
nltk_predictions = pd.read_csv("../data/gender_predictions/nltk_gender_predictions.csv")
nltk_predictions.sort_values(by="Estimated_Accuracy", ascending=False)[["First_Name", "Sex_at_birth","Estimated_Accuracy"]].drop_duplicates().head(5)
```

To represent the extra uncertainty in using a classifier compared to the babyname corpus, the `Estimated_Accuracy` column for NLTK predictions is the classifier's predict-proba score multplied by `{python} accuracy`. Where `{python} accuracy` is the accuracy of the classifier on the test set.

Like with the babyname corpus predictions, NLTK predictions with an accuracy less than 0.8 were given a gender prediction of `None`. This was the case if the predict-proba score from the classifier was less than `{python} round(0.8/accuracy,2)`.

```{python}
nltk_predictions = pd.read_csv("../data/gender_predictions/nltk_gender_predictions.csv")
accurate_nltk_predictions = nltk_predictions[nltk_predictions['Estimated_Accuracy'] >= 0.8]
nltk_kept_preds = round(100*accurate_nltk_predictions.shape[0]/(nltk_predictions.shape[0]),1)
```

Overall, `{python} nltk_kept_preds`% of the NLTK predictions were kept and the rest were given a prediction of `None`.

## Exploratory Data Analysis


## Limitations

**Due to the low level of accuracy in gender predicion of this report, conlusions drawn are not meaningful.**

- NLTK training data not representative of real data
- no practical way to check ground truths
- data sources
- gender not binary
- Naive Bayes conditional independence assumption violated
  These features are correlated which violates the Naive Bayes conditional independence assumption. However, I decided to stick with the NLTK naive bayes classifier as it efficiently handles text data, large datasets, and still performs fairly well.
- etc.

## References